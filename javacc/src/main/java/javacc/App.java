/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package javacc;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.parquet.example.data.Group;
import org.apache.parquet.hadoop.ParquetFileReader;
import org.apache.parquet.hadoop.example.GroupReadSupport;
import org.apache.parquet.hadoop.util.HadoopInputFile;
import org.apache.parquet.io.ColumnIOFactory;
import org.apache.parquet.io.MessageColumnIO;
import org.apache.parquet.io.RecordReader;
import org.apache.parquet.schema.MessageType;
import org.apache.parquet.hadoop.api.ReadSupport.ReadContext;
import java.io.IOException;

public class App {
  public static void main(String[] args) {
    try {
      Configuration conf = new Configuration();
      Path path = new Path(args[0]);
      HadoopInputFile inputFile = HadoopInputFile.fromPath(path, conf);

      ParquetFileReader reader = ParquetFileReader.open(inputFile);
      MessageType schema = reader.getFooter().getFileMetaData().getSchema();
      GroupReadSupport readSupport = new GroupReadSupport();
      ReadContext readContext = readSupport.init(conf, null, schema);

      MessageColumnIO columnIO = new ColumnIOFactory().getColumnIO(schema);
      RecordReader<Group> recordReader = columnIO.getRecordReader(reader.readNextRowGroup(),
          readSupport.prepareForRead(conf, null, schema, readContext));

      Group group;
      int count = 0;
      while (count < 10 && (group = recordReader.read()) != null) {
        System.out.println(group.toString());
        count++;
      }

      reader.close();
    } catch (IOException e) {
      e.printStackTrace();
    }
  }
}
